{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00bc6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4a0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/06 06:12:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2ba4e",
   "metadata": {},
   "source": [
    "<b>Question 1: \\\n",
    "Execute spark.version.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3e4c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c55f0ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-06 07:26:27--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230306T072627Z&X-Amz-Expires=300&X-Amz-Signature=1f0038be82f1e3989ae3027a87c4e296b04c81bb9d6d98b35af45553bc850278&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-06 07:26:27--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230306T072627Z&X-Amz-Expires=300&X-Amz-Signature=1f0038be82f1e3989ae3027a87c4e296b04c81bb9d6d98b35af45553bc850278&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 175799316 (168M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-06.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 167.66M  8.62MB/s    in 19s     \n",
      "\n",
      "2023-03-06 07:26:47 (8.76 MB/s) - ‘fhvhv_tripdata_2021-06.csv.gz’ saved [175799316/175799316]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10e33b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651315 fhvhv_tripdata_2021-06.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5236cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 paul paul 878M Dec 20 00:13 fhvhv_tripdata_2021-06.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv_tripdata_2021-06.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a3399a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68bc8b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv')\n",
    "\n",
    "df = df.repartition(12)\n",
    "\n",
    "df.write.parquet('data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2abb07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 55:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "parquet_dir = \"data/pq/fhvhv/2021/06/\"\n",
    "rdd = sc.binaryFiles(parquet_dir)\n",
    "num_files = rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6043d16",
   "metadata": {},
   "source": [
    "<b>Question 2: \\\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39ecc0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size: 22.549726963043213 MB\n"
     ]
    }
   ],
   "source": [
    "avg_size = rdd.map(lambda x: len(x[1])).reduce(lambda x, y: x + y) / (num_files * 1024 * 1024)\n",
    "print(\"Average size:\", avg_size, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58989b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6bf2bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02617|2021-06-04 16:50:34|2021-06-04 17:01:18|         118|         109|      N|                B02617|\n",
      "|              B02875|2021-06-02 22:28:45|2021-06-02 22:37:28|         163|          79|      N|                B02875|\n",
      "|              B02871|2021-06-03 11:47:48|2021-06-03 11:52:23|         231|          13|      N|                B02871|\n",
      "|              B02888|2021-06-03 08:45:25|2021-06-03 09:00:12|           9|          92|      N|                B02888|\n",
      "|              B02510|2021-06-05 09:50:43|2021-06-05 10:06:53|          14|         133|      N|                  null|\n",
      "|              B02764|2021-06-03 22:55:50|2021-06-03 23:21:24|         152|          74|      N|                B02764|\n",
      "|              B02889|2021-06-02 03:15:48|2021-06-02 03:26:20|         220|         235|      N|                B02889|\n",
      "|              B02872|2021-06-01 11:50:42|2021-06-01 12:00:36|         162|         161|      N|                B02872|\n",
      "|              B02510|2021-06-04 06:51:45|2021-06-04 06:56:26|         206|         206|      N|                  null|\n",
      "|              B02835|2021-06-02 01:21:31|2021-06-02 01:50:23|          49|         182|      N|                B02835|\n",
      "|              B02510|2021-06-02 17:11:31|2021-06-02 18:18:32|         236|          89|      N|                  null|\n",
      "|              B02869|2021-06-01 18:48:20|2021-06-01 18:54:59|         151|          43|      N|                B02869|\n",
      "|              B02510|2021-06-02 16:16:53|2021-06-02 16:39:03|         181|         189|      N|                  null|\n",
      "|              B02510|2021-06-03 21:23:35|2021-06-03 21:33:33|         181|          89|      N|                  null|\n",
      "|              B02764|2021-06-01 06:51:24|2021-06-01 06:58:09|          86|          86|      N|                B02764|\n",
      "|              B02867|2021-06-04 18:26:09|2021-06-04 18:41:43|         162|         263|      N|                B02867|\n",
      "|              B02510|2021-06-01 10:21:49|2021-06-01 11:08:24|         151|          17|      N|                  null|\n",
      "|              B02682|2021-06-04 01:45:17|2021-06-04 01:53:16|         125|         164|      N|                B02682|\n",
      "|              B02875|2021-06-01 14:06:42|2021-06-01 14:33:53|         192|           7|      N|                B02875|\n",
      "|              B02510|2021-06-03 21:15:15|2021-06-03 21:21:24|         171|          16|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b01d2f",
   "metadata": {},
   "source": [
    "**Q3**: <b>How many taxi trips were there on February 15? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7489aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c2500fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd7ae60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhvhv_2021_06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d47c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 62:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_2021_06\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2021-06-15 00:00:00';\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f533b",
   "metadata": {},
   "source": [
    "**Q4**: <b>Longest trip for each day \\\n",
    "Now calculate the duration for each trip.\n",
    "How long was the longest trip in Hours?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7befe422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2eb6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max, round, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "279d9161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 77:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+\n",
      "|pickup_date|     max(duration)|max_duration_rounded|\n",
      "+-----------+------------------+--------------------+\n",
      "| 2021-06-25| 66.87888888888888|               66.88|\n",
      "| 2021-06-22|25.549722222222222|               25.55|\n",
      "| 2021-06-27|19.980833333333333|               19.98|\n",
      "| 2021-06-26| 18.19722222222222|                18.2|\n",
      "| 2021-06-23|16.466944444444444|               16.47|\n",
      "+-----------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 77:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration', ((col('dropoff_datetime').cast('long') - col('pickup_datetime').cast('long')) / 60)/60) \\\n",
    "    .withColumn('pickup_date', to_date(col('pickup_datetime'))) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .withColumn('max_duration_rounded', round(col('max(duration)'), 2)) \\\n",
    "    .orderBy('max_duration_rounded', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74cf0e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 80:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25| 66.87888888888888|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26| 18.19722222222222|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "| 2021-06-24|13.909722222222223|\n",
      "| 2021-06-04|             11.67|\n",
      "| 2021-06-20|10.984444444444446|\n",
      "| 2021-06-01|           10.2675|\n",
      "| 2021-06-28|  9.96638888888889|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 80:==============>                                           (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX(((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60)/60) AS duration\n",
    "FROM \n",
    "    fhvhv_2021_06\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915096b",
   "metadata": {},
   "source": [
    "**Q5**: User Interface\n",
    "\n",
    "<b>Spark’s User Interface which shows application's dashboard runs on which local port?</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c52db",
   "metadata": {},
   "source": [
    "Answer: <b>4040</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10173a",
   "metadata": {},
   "source": [
    "**Q6**: <b>Most frequent pickup location zone.</b> \\\n",
    "<b>Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74b7f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81642d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cea4cf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(LocationID='1', Borough='EWR', Zone='Newark Airport', service_zone='EWR'),\n",
       " Row(LocationID='2', Borough='Queens', Zone='Jamaica Bay', service_zone='Boro Zone'),\n",
       " Row(LocationID='3', Borough='Bronx', Zone='Allerton/Pelham Gardens', service_zone='Boro Zone')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f460dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad8f0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f738414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 92:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|             pu_loc|count(1)|\n",
      "+-------------------+--------+\n",
      "|Crown Heights North|  231279|\n",
      "|       East Village|  221244|\n",
      "|        JFK Airport|  188867|\n",
      "|     Bushwick South|  187929|\n",
      "|      East New York|  186780|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    CONCAT(pul.Zone) AS pu_loc,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_2021_06 fhv INNER JOIN zones pul ON fhv.PULocationID = pul.LocationID\n",
    "                      \n",
    "GROUP BY \n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad7e50",
   "metadata": {},
   "source": [
    "<b>Answer: Crown Heights North</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff149c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
